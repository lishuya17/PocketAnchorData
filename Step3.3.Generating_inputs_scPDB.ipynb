{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pymol\n",
    "from pymol import cmd\n",
    "import os, time, sys, pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from collections import defaultdict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please download scPDB dataset first (http://bioinfo-pharma.u-strasbg.fr/scPDB/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Define dataset parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = 6\n",
    "scpdb_path = '/data/lishuya/software/scPDB/' # change this to your local path \n",
    "\n",
    "outdir = './PocketDetectionData_scPDB_da{}/'.format(da)\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9444"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepsurf_list = []\n",
    "with open('lists/deepsurf_scPDB.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        deepsurf_list.append(line.strip())\n",
    "len(deepsurf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9442"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdbid_chain_dict = {}\n",
    "with open('lists/pdbid_chain_list_scPDB.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        pdbid, chains = line.strip().split('_')\n",
    "        pdbid_chain_dict[pdbid] = chains\n",
    "len(pdbid_chain_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16612"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdbid_to_folder = defaultdict(lambda: [])\n",
    "for fname in os.listdir(scpdb_path):\n",
    "    pdbid_to_folder[fname[:4]].append(fname)\n",
    "len(pdbid_to_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get protein features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9442/9442, time 109.02 s\n",
      "anchor_dict 9436\n"
     ]
    }
   ],
   "source": [
    "anchor_dict = {}\n",
    "i = 0\n",
    "total = len(pdbid_chain_dict)\n",
    "t1 = time.time()\n",
    "\n",
    "for pdbid in pdbid_chain_dict:\n",
    "    chains = pdbid_chain_dict[pdbid]\n",
    "    i += 1\n",
    "    print('{}/{}, time {:.2f} s\\r'.format(i, total, time.time()-t1), end=\"\")\n",
    "\n",
    "    try:\n",
    "        anchor_dict[pdbid] = np.load('AnchorOutput/{}_{}_da_{}/anchors.npy'.format(pdbid, chains, da))[0]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\nanchor_dict\", len(anchor_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9442/9442, time 177.07 s\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9238"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom_dict = {}\n",
    "\n",
    "i = 0\n",
    "t1 = time.time()\n",
    "total = len(pdbid_chain_dict)\n",
    "for pdbid in pdbid_chain_dict:\n",
    "    chains = pdbid_chain_dict[pdbid]\n",
    "    i += 1\n",
    "    print('{}/{}, time {:.2f} s\\r'.format(i, total, time.time()-t1), end=\"\")\n",
    "\n",
    "    try:\n",
    "        repeat_list = pickle.load(open('AnchorOutput/{}_{}_da_{}/atom_feature.pk'\\\n",
    "                                          .format(pdbid, chains, da), 'rb'))[0]\n",
    "        assert len(repeat_list[0]) != 0\n",
    "        atom_dict[pdbid] = repeat_list\n",
    "    except:\n",
    "        pass\n",
    "len(atom_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outdir+'atom_feature_coord_nei_dict_thre'+str(da), \"wb\") as f:\n",
    "    pickle.dump(atom_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pdbid in atom_dict:\n",
    "#     if len(atom_dict[pdbid][0]) == 0:\n",
    "#         print(pdbid)\n",
    "# # atom_dict[pdbid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9442/9442, time 179.31 s\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9239"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masif_feature_coord_nei_dict = {}\n",
    "i = 0\n",
    "t1 = time.time()\n",
    "\n",
    "for pdbid in pdbid_chain_dict:\n",
    "    chains = pdbid_chain_dict[pdbid]\n",
    "    i += 1\n",
    "    print('{}/{}, time {:.2f} s\\r'.format(i, total, time.time()-t1), end=\"\")\n",
    "\n",
    "    try:\n",
    "        masif_feature = np.load('AnchorOutput/{}_{}_da_{}/masif_feature.npy'.format(pdbid, chains, da))\n",
    "        masif_neighbor = np.load('AnchorOutput/{}_{}_da_{}/masif_neighbor.npy'.format(pdbid, chains, da))\n",
    "        masif_coords = []\n",
    "        masif_feature_coord_nei_dict[pdbid] = (masif_feature, masif_coords, masif_neighbor)\n",
    "    except:\n",
    "        pass\n",
    "len(masif_feature_coord_nei_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_dict = {}\n",
    "aa_dict = {}\n",
    "at_dict = {}\n",
    "i = 0\n",
    "t1 = time.time()\n",
    "\n",
    "for pdbid in pdbid_chain_dict:\n",
    "    chains = pdbid_chain_dict[pdbid]\n",
    "    i += 1\n",
    "    if i % 100 == 0:\n",
    "        print('{}/{}, time {:.2f} s\\r'.format(i, total, time.time()-t1), end=\"\")\n",
    "    try:  \n",
    "        am = torch.load('AnchorOutput/{}_{}_da_{}/am_list.pt'.format(pdbid, chains, da))[0]\n",
    "        at = torch.load('AnchorOutput/{}_{}_da_{}/at_list.pt'.format(pdbid, chains, da))[0]\n",
    "        aa = torch.load('AnchorOutput/{}_{}_da_{}/aa_list.pt'.format(pdbid, chains, da))[0]\n",
    "        am_dict[pdbid] = am\n",
    "        at_dict[pdbid] = at\n",
    "        aa_dict[pdbid] = aa\n",
    "    except Exception as E:\n",
    "        print(pdbid, E)\n",
    "        print(\n",
    "            os.path.exists('AnchorOutput/{}_{}_da_{}/am_list.pt'.format(pdbid, chains, da)), \n",
    "            os.path.exists('AnchorOutput/{}_{}_da_{}/at_list.pt'.format(pdbid, chains, da)), \n",
    "            os.path.exists('AnchorOutput/{}_{}_da_{}/aa_list.pt'.format(pdbid, chains, da)), \n",
    "             )\n",
    "        pass\n",
    "    \n",
    "print('\\n', len(am_dict), len(aa_dict), len(at_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outdir+'anchor_dict_thre'+str(da), 'wb') as f:\n",
    "    pickle.dump(anchor_dict, f)\n",
    "    \n",
    "with open(outdir+'atom_feature_coord_nei_dict_thre'+str(da), \"wb\") as f:\n",
    "    pickle.dump(atom_dict, f)\n",
    "\n",
    "with open(outdir+'masif_feature_coord_nei_dict', 'wb') as f:\n",
    "    pickle.dump(masif_feature_coord_nei_dict, f)\n",
    "    \n",
    "with open(outdir+'am_dict', 'wb') as f:\n",
    "    pickle.dump(am_dict, f)\n",
    "    \n",
    "with open(outdir+'aa_dict', 'wb') as f:\n",
    "    pickle.dump(aa_dict, f)\n",
    "    \n",
    "with open(outdir+'at_dict', 'wb') as f:\n",
    "    pickle.dump(at_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. get label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ligand_counts_coords(pdbid, chains, removeHs=True):\n",
    "    if os.path.exists('MasifOutput/00-raw_pdbs/fixed_{}.pdb'.format(pdbid)):\n",
    "        filename = 'MasifOutput/00-raw_pdbs/fixed_{}.pdb'.format(pdbid)\n",
    "        protein = 'fixed_'+pdbid\n",
    "    elif os.path.exists('MasifOutput/00-raw_pdbs/{}.pdb'.format(pdbid)):\n",
    "        filename = 'MasifOutput/00-raw_pdbs/{}.pdb'.format(pdbid)\n",
    "        protein = pdbid\n",
    "    else:\n",
    "        print(\"NO pdb file\", pdbid)\n",
    "        return None\n",
    "#     print(filename)\n",
    "    pymol.cmd.reinitialize()\n",
    "    cmd.load(filename)\n",
    "    for folder in pdbid_to_folder[pdbid]:\n",
    "        cmd.load(scpdb_path+'{}/ligand.mol2'.format(folder))\n",
    "    if removeHs:\n",
    "        pymol.cmd.remove('hydro')\n",
    "    protein_coords = []\n",
    "    pymol.cmd.iterate_state(-1, \"{} and chain {} and not het\".format(protein, '+'.join(chains)),\n",
    "                            \"protein_coords.append((x,y,z))\", space=locals())\n",
    "    ligand_list = set()\n",
    "    pymol.cmd.iterate('ligand*', 'ligand_list.add(resn)', space=locals())    \n",
    "#     print('ligand_list', ligand_list)\n",
    "    count_dict, coord_dict = {}, {}\n",
    "    list_tabu = [\"HOH\", \"DOD\", \"WAT\", \"NAG\", \"MAN\", \"GLC\", \"ABA\", \"MPD\", \"GOL\", \"SO4\", \"PO4\", '', 'U', 'HEM', 'PI']\n",
    "    list_ligand_ok = set()\n",
    "    for ligand in ligand_list:\n",
    "        if ligand in list_tabu:\n",
    "            continue\n",
    "        resi_set = set()\n",
    "        ligand = ligand.upper()\n",
    "        pymol.cmd.iterate('resname {}'.format(ligand), \"resi_set.add(chain+'_'+resi)\", space=locals())\n",
    "#         print('resi_set', resi_set)\n",
    "        count_dict[ligand] = 0\n",
    "        for chain_resi in resi_set:\n",
    "            chain, resi = chain_resi.split('_')\n",
    "            pymol.cmd.select('{}_{}'.format(ligand, chain_resi), 'chain {} and resi {}'.format(chain, resi))\n",
    "            coords = []\n",
    "            pymol.cmd.iterate_state(-1, '{}_{}'.format(ligand, chain_resi), \"coords.append((x,y,z))\", space=locals())\n",
    "            if len(coords) < 5:\n",
    "                continue\n",
    "            coords = np.array(coords) \n",
    "#             print('coords', np.max(coords, 0), np.min(coords, 0))\n",
    "#             print(len(protein_coords), len(coords), pairwise_distances(protein_coords, coords).min())\n",
    "            \n",
    "            if pairwise_distances(protein_coords, coords).min() < 1.5:\n",
    "                continue\n",
    "            if pairwise_distances(protein_coords, coords).min() > 4:\n",
    "                continue\n",
    "            if pairwise_distances(protein_coords, np.mean(coords, 0, keepdims=True)).min() > 5.5:\n",
    "                continue\n",
    "            coord_dict['{}_{}'.format(ligand, chain_resi)] = coords\n",
    "            count_dict[ligand] += 1\n",
    "            list_ligand_ok.add(ligand)\n",
    "    return count_dict, coord_dict, list(list_ligand_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ligand_counts_coords('3uzx', 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PyMOL not running, entering library mode (experimental)\n",
      "557/9436, time 130.74 s\n",
      "NO LIGAND 557 1rml\n",
      "1984/9436, time 289.65 s\n",
      "NO LIGAND 1984 2iyf\n",
      "1992/9436, time 290.13 s\n",
      "NO LIGAND 1992 2j0d\n",
      "2092/9436, time 306.08 s\n",
      "NO LIGAND 2092 2kwi\n",
      "2111/9436, time 310.70 s\n",
      "NO LIGAND 2111 2mse\n",
      "2254/9436, time 325.67 s\n",
      "NO LIGAND 2254 2oxn\n",
      "2964/9436, time 404.80 s\n",
      "NO LIGAND 2964 2xup\n",
      "3361/9436, time 440.16 s\n",
      "NO LIGAND 3361 3au2\n",
      "3550/9436, time 459.43 s\n",
      "NO LIGAND 3550 3ck7\n",
      "4720/9436, time 591.93 s\n",
      "NO LIGAND 4720 3nu5\n",
      "5108/9436, time 636.79 s\n",
      "NO LIGAND 5108 3r0y\n",
      "5401/9436, time 671.98 s\n",
      "NO LIGAND 5401 3tfj\n",
      "5415/9436, time 673.73 s\n",
      "NO LIGAND 5415 3tjs\n",
      "5524/9436, time 686.53 s\n",
      "NO LIGAND 5524 3ug8\n",
      "5526/9436, time 686.60 s\n",
      "NO LIGAND 5526 3ugr\n",
      "5923/9436, time 742.58 s\n",
      "NO LIGAND 5923 3zhz\n",
      "6727/9436, time 842.27 s\n",
      "NO LIGAND 6727 4ef4\n",
      "6867/9436, time 859.53 s\n",
      "NO LIGAND 6867 4f9g\n",
      "7373/9436, time 926.11 s\n",
      "NO LIGAND 7373 4ig8\n",
      "7857/9436, time 1002.09 s\n",
      "NO LIGAND 7857 4lnb\n",
      "7924/9436, time 1013.72 s\n",
      "NO LIGAND 7924 4m83\n",
      "8084/9436, time 1038.35 s\n",
      "NO LIGAND 8084 4nmj\n",
      "8085/9436, time 1038.76 s\n",
      "NO LIGAND 8085 4nmk\n",
      "8086/9436, time 1039.17 s\n",
      "NO LIGAND 8086 4npt\n",
      "8677/9436, time 1120.06 s\n",
      "NO LIGAND 8677 4wdb\n",
      "9125/9436, time 1182.77 s\n",
      "NO LIGAND 9125 5cfn\n",
      "9155/9436, time 1186.84 s\n",
      "NO LIGAND 9155 5d4g\n",
      "count_dict_all 9409 coord_dict_all 9409\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "total = len(anchor_dict)\n",
    "t1 = time.time()\n",
    "\n",
    "count_dict_all = {}\n",
    "coord_dict_all = {}\n",
    "for pdbid in anchor_dict:\n",
    "    i += 1\n",
    "    chains = pdbid_chain_dict[pdbid]\n",
    "    print('{}/{}, time {:.2f} s\\r'.format(i, total, time.time()-t1), end=\"\")\n",
    "    try:\n",
    "        count_dict, coord_dict, list_ligand = get_ligand_counts_coords(pdbid, list(chains))\n",
    "        if len(list_ligand) == 0:\n",
    "            print('\\nNO LIGAND {} {}'.format(i, pdbid))\n",
    "            continue\n",
    "        count_dict_all[pdbid] = count_dict\n",
    "        coord_dict_all[pdbid] = coord_dict\n",
    "    except Exception as E:\n",
    "        print('\\n{} {} {}'.format(i, pdbid, E))\n",
    "print('count_dict_all', len(count_dict_all), 'coord_dict_all', len(coord_dict_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17244"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of ligands \n",
    "\n",
    "a = 0\n",
    "for x in count_dict_all.values():\n",
    "    a += sum(x.values())\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9409"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_dict_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 get anchor labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9408  \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9409"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single\n",
    "\n",
    "label_dict_single = {}\n",
    "processed = 0\n",
    "\n",
    "for pdbid in count_dict_all:\n",
    "    count_dict = count_dict_all[pdbid]\n",
    "    print(processed, \" \\r\", end='')\n",
    "    ligand_coords = coord_dict_all[pdbid]\n",
    "    anchor_coords = anchor_dict[pdbid]\n",
    "    cpd_coords = np.concatenate(list(ligand_coords.values()), axis=0)\n",
    "    ag = pairwise_distances(anchor_coords, cpd_coords).min(axis=1)\n",
    "    label = (ag <= 4).astype(int)\n",
    "    label_dict_single[pdbid] = label\n",
    "    processed += 1\n",
    "\n",
    "len(label_dict_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outdir+'label_dict_single', 'wb') as f:\n",
    "    pickle.dump(label_dict_single, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save final dataset table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9209, 9209, 9209)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "total = len(list(anchor_dict.keys()))\n",
    "t1 = time.time()\n",
    "\n",
    "pdbid_list = []\n",
    "ligand_list = []\n",
    "num_ligand_list = []\n",
    "\n",
    "for pdbid in list(anchor_dict.keys()):\n",
    "    i += 1\n",
    "    chains = pdbid_chain_dict[pdbid]\n",
    "\n",
    "    if pdbid not in count_dict_all:\n",
    "        continue\n",
    "    count_dict = count_dict_all[pdbid]\n",
    "    ligands = ','.join(list(count_dict.keys()))\n",
    "    num_ligands = sum(count_dict.values())\n",
    "    \n",
    "    if pdbid not in anchor_dict:\n",
    "        continue\n",
    "    if pdbid not in atom_dict:\n",
    "        continue  \n",
    "    if pdbid not in masif_feature_coord_nei_dict:\n",
    "        continue  \n",
    "    if pdbid not in am_dict:\n",
    "        continue  \n",
    "    if pdbid not in at_dict:\n",
    "        continue  \n",
    "    if pdbid not in aa_dict:\n",
    "        continue     \n",
    "    if pdbid not in label_dict_single:\n",
    "        continue  \n",
    "    pdbid_list.append(pdbid)\n",
    "    ligand_list.append(ligands)\n",
    "    num_ligand_list.append(num_ligands)\n",
    "    \n",
    "len(pdbid_list), len(ligand_list), len(num_ligand_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_single = pd.DataFrame()\n",
    "table_single['pdbid'] = pdbid_list\n",
    "table_single['ligand'] = ligand_list\n",
    "table_single['num_ligand'] = num_ligand_list\n",
    "table_single['chains'] = [pdbid_chain_dict[pdbid] for pdbid in pdbid_list]\n",
    "\n",
    "table_single.to_csv(outdir+'scPDB_table_pocket_single.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate protein similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasta_str(pdbid, chains=None, removeHs=True, debug=False):\n",
    "    if os.path.exists('MasifOutput/00-raw_pdbs/fixed_{}.pdb'.format(pdbid)):\n",
    "        filename = 'MasifOutput/00-raw_pdbs/fixed_{}.pdb'.format(pdbid)\n",
    "        protein = 'fixed_'+pdbid\n",
    "    elif os.path.exists('MasifOutput/00-raw_pdbs/{}.pdb'.format(pdbid)):\n",
    "        filename = 'MasifOutput/00-raw_pdbs/{}.pdb'.format(pdbid)\n",
    "        protein = pdbid\n",
    "    else:\n",
    "        print(\"NO pdb file\", pdbid)\n",
    "        return None\n",
    "    \n",
    "    pymol.cmd.reinitialize()\n",
    "    pymol.cmd.load(filename)\n",
    "    fasta_str = pymol.cmd.get_fastastr(protein + ' and chain '+chains)\n",
    "    if len(fasta_str) == 0:\n",
    "        return None\n",
    "    \n",
    "    fasta_str = fasta_str.replace('?', 'X')\n",
    "    # print(fasta_str)\n",
    "    seq_dict = {}\n",
    "    for i, s in enumerate(fasta_str.split('\\n')):\n",
    "        if s.startswith('>'):\n",
    "            name = s\n",
    "            if i != 0:\n",
    "                seq_dict[name] = seq\n",
    "            seq = \"\"\n",
    "        else:\n",
    "            seq += s \n",
    "    seq_dict[name] = seq\n",
    "    \n",
    "    seq_set = set()\n",
    "    new_fasta_str = \"\"\n",
    "    for name, seq in seq_dict.items():\n",
    "        if seq not in seq_set and len(seq) >= 15:\n",
    "            new_fasta_str += name + '\\n' + seq + '\\n'\n",
    "            seq_set.add(seq)\n",
    "    \n",
    "    if len(new_fasta_str) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return new_fasta_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if da==6:\n",
    "    fw = open('coach420_seq.fasta', 'w')\n",
    "    coach_table = pd.read_csv('PocketDetectionData_COACH420_da6/coach420_table_pocket_full.tsv', sep='\\t')\n",
    "    print(coach_table.shape)\n",
    "    done = set()\n",
    "    for i in coach_table.index:\n",
    "        print('{}/{}\\r'.format(i, len(coach_table)), end=\"\")\n",
    "        pdbid, chains = coach_table.loc[i, ['pdbid', 'eval_chains']]\n",
    "        if pdbid not in done:\n",
    "            done.add(pdbid)\n",
    "        else:\n",
    "            continue\n",
    "        fasta_str = get_fasta_str(pdbid, chains)\n",
    "        if fasta_str is not None:\n",
    "            fw.write(fasta_str)\n",
    "    # fw.write('\\n')\n",
    "    fw.close()\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if da==6:\n",
    "    holo_table = pd.read_csv('PocketDetectionData_HOLO4k_da6/holo4k_table_pocket_full.tsv', sep='\\t')\n",
    "    print(holo_table.shape)\n",
    "    fw = open('holo4k_seq.fasta', 'w')\n",
    "\n",
    "    done = set()\n",
    "    for i in holo_table.index:\n",
    "        print('{}/{}\\r'.format(i, len(holo_table)), end=\"\")\n",
    "        pdbid, chains = holo_table.loc[i, ['pdbid', 'chains']]\n",
    "        if pdbid not in done:\n",
    "            done.add(pdbid)\n",
    "        else:\n",
    "            continue\n",
    "        fasta_str = get_fasta_str(pdbid, chains)\n",
    "        if fasta_str is not None:\n",
    "            fw.write(fasta_str)\n",
    "    # fw.write('\\n')\n",
    "    fw.close()\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if da==6:\n",
    "    fw = open('scpdb_seq.fasta', 'w')\n",
    "\n",
    "    done = set()\n",
    "    for i in table_single.index:\n",
    "        print('{}/{}\\r'.format(i, len(table_single)), end=\"\")\n",
    "        pdbid, chains = table_single.loc[i, ['pdbid', 'chains']]\n",
    "        if pdbid not in done:\n",
    "            done.add(pdbid)\n",
    "        else:\n",
    "            continue\n",
    "        fasta_str = get_fasta_str(pdbid, chains)\n",
    "        if fasta_str is not None:\n",
    "            fw.write(fasta_str)\n",
    "    # fw.write('\\n')\n",
    "    fw.close()\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv *.fasta ./smith-waterman-src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# Then run in cmdline:\n",
    "\n",
    "```\n",
    "cd ./smith-waterman-src/\n",
    "\n",
    "chmod +x ssw_test\n",
    "\n",
    "./ssw_test -p coach420_seq.fasta scpdb_seq.fasta > coach_scpdb.out & \n",
    "\n",
    "./ssw_test -p holo4k_seq.fasta scpdb_seq.fasta > holo_scpdb.out &\n",
    "\n",
    "python pyssw_paired.py -p coach420_seq.fasta coach420_seq.fasta > coach.out\n",
    "\n",
    "python pyssw_paired.py -p holo4k_seq.fasta holo4k_seq.fasta > holo.out\n",
    "\n",
    "python pyssw_paired.py -p scpdb_seq.fasta scpdb_seq.fasta > scpdb.out\n",
    "```\n",
    "\n",
    "\\# When finishing the above 5 jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_symmetric(a, tol=1e-8):\n",
    "    return np.allclose(a, a.T, atol=tol)\n",
    "\n",
    "def get_protein_vocabulary_dict(fasta_name):\n",
    "    protein_vocabulary_dict = {}\n",
    "    f = open(fasta_name)\n",
    "    for line in f.readlines():\n",
    "        if line[0] == '>':\n",
    "            protein_vocabulary_dict[line[1:-1]] = len(protein_vocabulary_dict)\n",
    "    f.close()\n",
    "    print('protein_vocabulary_dict', len(protein_vocabulary_dict))\n",
    "    return protein_vocabulary_dict\n",
    "\n",
    "def sim_aquire(target_name, query_name, output_name):\n",
    "    '''without normalization'''\n",
    "    target_dict = get_protein_vocabulary_dict(target_name)\n",
    "    query_dict = get_protein_vocabulary_dict(query_name)\n",
    "    p_simi = np.zeros((len(target_dict), len(query_dict)))\n",
    "    \n",
    "    # parse results\n",
    "    f = open(output_name)\n",
    "    content = f.read()\n",
    "    content = content.replace(\"When maskLen < 15, the function ssw_align doesn't return 2nd best alignment information.\\n\", \"\")\n",
    "    content = content.replace(\"269\\n\\n\\n\", \"\")\n",
    "    lines = content.split('\\n')[:-1] ## ???? 不懂但有用\n",
    "#     lines = f.readlines()\n",
    "#     lines = [line for line in lines if not line.startswith('When ')]\n",
    "    f.close()\n",
    "    print('total lines', len(lines), len(lines)/4/p_simi.size)\n",
    "    for i in range(0,len(lines),4):\n",
    "        try:\n",
    "            a = lines[i].strip('\\n').split(' ')[-1]\n",
    "            b = lines[i+1].strip('\\n').split(' ')[-1]\n",
    "            c = float(int(lines[i+2].strip('\\n').split( )[1]))\n",
    "            p_simi[target_dict[a], query_dict[b]] = c\n",
    "        except Exception as E:\n",
    "            print(lines[i-4:i+4])\n",
    "            print('wrong', i, a, b, c)\n",
    "            print(E)\n",
    "            return\n",
    "            \n",
    "#     assert check_symmetric(p_simi)\n",
    "    \n",
    "    # normalize\n",
    "#     for i in range(p_simi.shape[0]):\n",
    "#         for j in range(p_simi.shape[0]):\n",
    "#             if i == j:\n",
    "#                 continue\n",
    "#             p_simi[i,j] = p_simi[i,j] / (float(np.sqrt(p_simi[i,i])*np.sqrt(p_simi[j,j]))+1e-12)\n",
    "#     for i in range(len(p_simi)):\n",
    "#         p_simi[i,i] = p_simi[i,i] / float(np.sqrt(p_simi[i,i])*np.sqrt(p_simi[i,i]))\n",
    "#     print('p_simi', p_simi.shape)\n",
    "#     assert check_symmetric(p_simi)\n",
    "    \n",
    "    target_list = ['']*len(target_dict)\n",
    "    for pid, idx in target_dict.items():\n",
    "        target_list[idx] = pid\n",
    "    print('target_list', len(target_list))\n",
    "    \n",
    "    query_list = ['']*len(query_dict)\n",
    "    for pid, idx in query_dict.items():\n",
    "        query_list[idx] = pid\n",
    "    print('query_list', len(query_list))\n",
    "    # assert target_list == query_list\n",
    "    return p_simi, target_list, query_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protein_vocabulary_dict 419\n",
      "protein_vocabulary_dict 4464\n",
      "total lines 7481664 1.0\n",
      "target_list 419\n",
      "query_list 4464\n",
      "protein_vocabulary_dict 2011\n",
      "protein_vocabulary_dict 4464\n",
      "total lines 35908416 1.0\n",
      "target_list 2011\n",
      "query_list 4464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((419, 4464), 0)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coach_scpdb_sim_mat, coach_list, scpdb_list = sim_aquire('./smith-waterman-src/coach420_seq.fasta', \n",
    "                                              './smith-waterman-src/scpdb_seq.fasta',\n",
    "                                              './smith-waterman-src/coach_scpdb.out')\n",
    "holo_scpdb_sim_mat, holo_list, scpdb_list = sim_aquire('./smith-waterman-src/holo4k_seq.fasta', \n",
    "                                              './smith-waterman-src/scpdb_seq.fasta',\n",
    "                                              './smith-waterman-src/holo_scpdb.out')\n",
    "coach_scpdb_sim_mat.shape, (coach_scpdb_sim_mat == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_aquire(target_list, output_name):\n",
    "    p_simi = {} # np.zeros(len(target_dict))\n",
    "    \n",
    "    # parse results\n",
    "    f = open(output_name)\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    print('total lines', len(lines))\n",
    "    for i in range(0,len(lines),4):\n",
    "        try:\n",
    "            a = lines[i].strip('\\n').split(' ')[-1]\n",
    "            b = lines[i+1].strip('\\n').split(' ')[-1]\n",
    "            assert a==b\n",
    "            c = float(int(lines[i+2].strip('\\n').split( )[1]))\n",
    "            p_simi[a] = c\n",
    "        except:\n",
    "            print(lines[i:i+4])\n",
    "            print('wrong', i, a, b, c)\n",
    "            \n",
    "    res = []    \n",
    "    for target in target_list:\n",
    "        if target in p_simi:\n",
    "            res.append(p_simi[target])\n",
    "        else:\n",
    "            res.append(0)\n",
    "    \n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines 1676\n",
      "total lines 8044\n",
      "total lines 17856\n",
      "(419,) (2011,) (4464,)\n"
     ]
    }
   ],
   "source": [
    "coach_reg = reg_aquire(coach_list, './smith-waterman-src/coach.out')\n",
    "holo_reg = reg_aquire(holo_list, './smith-waterman-src/holo.out')\n",
    "scpdb_reg = reg_aquire(scpdb_list, './smith-waterman-src/scpdb.out')\n",
    "print(coach_reg.shape, holo_reg.shape, scpdb_reg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coach_scpdb_sim_norm (419, 4464)\n"
     ]
    }
   ],
   "source": [
    "coach_scpdb_sim_norm = np.zeros(coach_scpdb_sim_mat.shape)\n",
    "for i in range(coach_scpdb_sim_mat.shape[0]):\n",
    "    for j in range(coach_scpdb_sim_mat.shape[1]):\n",
    "        coach_scpdb_sim_norm[i,j] = coach_scpdb_sim_mat[i,j] / (float(np.sqrt(coach_reg[i])*np.sqrt(scpdb_reg[j]))+1e-12)\n",
    "print('coach_scpdb_sim_norm', coach_scpdb_sim_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holo_scpdb_sim_norm (2011, 4464)\n"
     ]
    }
   ],
   "source": [
    "holo_scpdb_sim_norm = np.zeros(holo_scpdb_sim_mat.shape)\n",
    "for i in range(holo_scpdb_sim_mat.shape[0]):\n",
    "    for j in range(holo_scpdb_sim_mat.shape[1]):\n",
    "        holo_scpdb_sim_norm[i,j] = holo_scpdb_sim_mat[i,j] / (float(np.sqrt(holo_reg[i])*np.sqrt(scpdb_reg[j]))+1e-12)\n",
    "print('holo_scpdb_sim_norm', holo_scpdb_sim_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9209, 9209)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_pdbid_list = table_single['pdbid'].values\n",
    "table_pdbid_dict = {pdbid:i for i,pdbid in enumerate(table_pdbid_list)}\n",
    "len(table_pdbid_list), len(table_pdbid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14330859983664965"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxsim_coach = np.zeros(len(table_pdbid_dict))\n",
    "for coli, item in enumerate(scpdb_list):\n",
    "    pdbid = item[:4]\n",
    "    if pdbid not in table_pdbid_dict:\n",
    "        continue\n",
    "    idx = table_pdbid_dict[pdbid]\n",
    "    maxsim = max(maxsim_coach[idx], coach_scpdb_sim_norm[:,coli].max())\n",
    "    maxsim_coach[idx] = maxsim\n",
    "np.mean(maxsim_coach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1722204401618667"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxsim_holo = np.zeros(len(table_pdbid_dict))\n",
    "for coli, item in enumerate(scpdb_list):\n",
    "    pdbid = item[:4]\n",
    "    if pdbid not in table_pdbid_dict:\n",
    "        continue\n",
    "    idx = table_pdbid_dict[pdbid]\n",
    "    maxsim = max(maxsim_holo[idx], holo_scpdb_sim_norm[:,coli].max())\n",
    "    maxsim_holo[idx] = maxsim\n",
    "np.mean(maxsim_holo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9209,)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxsim_both = np.vstack([maxsim_holo, maxsim_coach]).max(0)\n",
    "maxsim_both.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_single['maxsim'] = maxsim_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_single['sim0.9'] = (maxsim_both <= 0.9).astype(bool)\n",
    "table_single['sim0.8'] = (maxsim_both <= 0.8).astype(bool)\n",
    "table_single['sim0.7'] = (maxsim_both <= 0.7).astype(bool)\n",
    "table_single['sim0.6'] = (maxsim_both <= 0.6).astype(bool)\n",
    "table_single['sim0.5'] = (maxsim_both <= 0.5).astype(bool)\n",
    "table_single['sim0.4'] = (maxsim_both <= 0.4).astype(bool)\n",
    "table_single['sim0.3'] = (maxsim_both <= 0.3).astype(bool)\n",
    "table_single['sim0.2'] = (maxsim_both <= 0.2).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_single.to_csv(outdir+'scPDB_table_pocket_single_similarity.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
