{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, pickle, sys, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from pymol import cmd\n",
    "from sklearn.metrics import pairwise_distances, pairwise_distances_argmin_min\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.ML.Cluster import Butina\n",
    "from scipy.cluster.hierarchy import fcluster, linkage, single\n",
    "from scipy.spatial.distance import pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Define dataset parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = 6\n",
    "outdir = './PocketDetectionData_COACH420_da{}/'.format(da)\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(420, 3)\n"
     ]
    }
   ],
   "source": [
    "table = pd.read_csv(\"lists/coach420_pdbid_ligandname.csv\")\n",
    "dict_ligands = {}\n",
    "for i in table.index:\n",
    "    list_ligand = eval(table.loc[i, 'ligand_name'])\n",
    "    pdbid_chains = table.loc[i, 'pdbid']\n",
    "    table.loc[i, 'ligand_name'] = \",\".join(list_ligand)\n",
    "    table.loc[i, 'pdbid'] = pdbid_chains[:4]\n",
    "    table.loc[i, 'chains'] = pdbid_chains[4:]\n",
    "    dict_ligands[pdbid_chains[:4]] = \",\".join(list_ligand)\n",
    "table.columns = ['pdbid', 'ligand', 'eval_chains']\n",
    "print(table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdbid</th>\n",
       "      <th>ligand</th>\n",
       "      <th>eval_chains</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>148l</td>\n",
       "      <td>UUU</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1a26</td>\n",
       "      <td>CNA</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1a2k</td>\n",
       "      <td>GDP</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1a4k</td>\n",
       "      <td>FRA</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1a7x</td>\n",
       "      <td>FKA</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pdbid ligand eval_chains\n",
       "0  148l    UUU           E\n",
       "1  1a26    CNA           A\n",
       "2  1a2k    GDP           C\n",
       "3  1a4k    FRA           H\n",
       "4  1a7x    FKA           A"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_new_name = {\n",
    "    '2zcp': '3w7f',\n",
    "} # updated PDB ID\n",
    "\n",
    "def update_pdbid_chains(pdbid_chains):\n",
    "    pdbid, chains = pdbid_chains.split(\"_\")\n",
    "    if pdbid not in dict_new_name:\n",
    "        return pdbid_chains\n",
    "    newid = dict_new_name[pdbid]\n",
    "    return \"_\".join([newid, chains])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lists/pdbid_chain_list_COACH420.txt', 'r') as f:\n",
    "    list_task = f.readlines()\n",
    "list_task = [item.strip() for item in list_task]\n",
    "dict_chains = {}\n",
    "for task in list_task:\n",
    "    pdbid, chains = task.split(\"_\")\n",
    "    if pdbid in dict_chains:\n",
    "        dict_chains[pdbid].append(chains)\n",
    "    else:\n",
    "        dict_chains[pdbid] = [chains]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in table.index:\n",
    "    pdbid = table.loc[i, 'pdbid']\n",
    "    if pdbid in dict_new_name:\n",
    "        pdbid = dict_new_name[pdbid]\n",
    "    if pdbid not in dict_chains:\n",
    "        print(pdbid)\n",
    "        continue\n",
    "    table.loc[i, 'chains'] = \",\".join(dict_chains[pdbid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get protein features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_anchor(list_samples):\n",
    "    list_anchor = []\n",
    "    for pdbid_chains in list_samples:\n",
    "        pdbid_chains = update_pdbid_chains(pdbid_chains)\n",
    "        anchor = np.load('AnchorOutput/{}_da_{}/anchors.npy'.format(pdbid_chains, da))[0]\n",
    "        list_anchor.append(anchor)\n",
    "    return np.concatenate(list_anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchor_dict 420\n"
     ]
    }
   ],
   "source": [
    "anchor_dict = {}\n",
    "for i in table.index:\n",
    "    print(len(anchor_dict), \"\\r\", end=\"\")\n",
    "    pdbid = table.loc[i, 'pdbid']\n",
    "    try:\n",
    "        list_chains = [pdbid + \"_\" + chain for chain in table.loc[i, 'chains'].split(\",\")]\n",
    "        anchor_dict[pdbid] = load_anchor(list_chains)\n",
    "    except Exception as E:\n",
    "        print(i, E)\n",
    "        pass\n",
    "\n",
    "print(\"anchor_dict\", len(anchor_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outdir+'anchor_dict_thre'+str(da), 'wb') as f:\n",
    "    pickle.dump(anchor_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_atom_dict(list_filenames):\n",
    "    list_fa = []\n",
    "    list_coord = []\n",
    "    list_nei = []\n",
    "    count = 0\n",
    "    for pdbid_chains in list_filenames:\n",
    "        pdbid_chains = update_pdbid_chains(pdbid_chains)\n",
    "        fa, coord, nei = pickle.load(open('AnchorOutput/{}_da_{}/atom_feature.pk'\\\n",
    "                                          .format(pdbid_chains, da), 'rb'))[0]\n",
    "        list_fa.append(fa)\n",
    "        list_coord.append(coord)\n",
    "        list_nei.extend([[jtem + count for jtem in item] for item in nei])\n",
    "        count += len(fa)\n",
    "    list_fa = np.concatenate(list_fa)\n",
    "    list_coord = np.concatenate(list_coord)\n",
    "    return (list_fa, list_coord, list_nei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419 \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "420"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom_dict = {}\n",
    "\n",
    "for i in table.index:\n",
    "    print(len(atom_dict), \"\\r\", end=\"\")\n",
    "    pdbid = table.loc[i, 'pdbid']\n",
    "    try:\n",
    "        list_chains = [pdbid + \"_\" + chain for chain in table.loc[i, 'chains'].split(\",\")]\n",
    "        atom_dict[pdbid] = load_atom_dict(list_chains)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "len(atom_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outdir+\"atom_feature_coord_nei_dict_thre\"+str(da), \"wb\") as f:\n",
    "    pickle.dump(atom_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_masif_coords(list_filenames):\n",
    "    list_prt_coord = []\n",
    "    for pdbid_chains in list_filenames:\n",
    "        pdbid_chains = update_pdbid_chains(pdbid_chains)\n",
    "        masif_coords = np.vstack([\n",
    "            np.load('MasifOutput/04a-precomputation_12A/precomputation/{}/p1_X.npy'.format(pdbid_chains)),\n",
    "            np.load('MasifOutput/04a-precomputation_12A/precomputation/{}/p1_Y.npy'.format(pdbid_chains)),\n",
    "            np.load('MasifOutput/04a-precomputation_12A/precomputation/{}/p1_Z.npy'.format(pdbid_chains)),\n",
    "        ]).T\n",
    "        list_prt_coord.append(masif_coords)\n",
    "    return np.concatenate(list_prt_coord)\n",
    "\n",
    "\n",
    "def load_masif_feature_neighbor(list_filenames):\n",
    "    list_feat = []\n",
    "    list_nei = []\n",
    "    count = 0\n",
    "    for pdbid_chains in list_filenames:\n",
    "        pdbid_chains = update_pdbid_chains(pdbid_chains)\n",
    "        feat = np.load('AnchorOutput/{}_da_{}/masif_feature.npy'.format(pdbid_chains, da))\n",
    "        nei = np.load('AnchorOutput/{}_da_{}/masif_neighbor.npy'.format(pdbid_chains, da))\n",
    "        if np.isnan(feat).sum() != 0:\n",
    "            feat[np.where(np.isnan(feat))] = 0\n",
    "        list_feat.append(feat)\n",
    "        list_nei.append(nei + count)\n",
    "        count += len(feat)\n",
    "    return np.concatenate(list_feat), np.concatenate(list_nei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419 \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "420"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masif_feature_coord_nei_dict = {}\n",
    "\n",
    "for i in table.index:\n",
    "    print(len(masif_feature_coord_nei_dict), \"\\r\", end=\"\")\n",
    "    pdbid = table.loc[i, 'pdbid']\n",
    "    try:    \n",
    "        list_chains = [pdbid + \"_\" + chain for chain in table.loc[i, 'chains'].split(\",\")]    \n",
    "        masif_feature, masif_neighbor = load_masif_feature_neighbor(list_chains)\n",
    "        masif_coords = load_masif_coords(list_chains)\n",
    "        assert masif_feature.shape[0] == masif_coords.shape[0], \"{} {} {}\".format(pdbid, masif_feature.shape[0], masif_coords.shape[0])\n",
    "        masif_feature_coord_nei_dict[pdbid] = (masif_feature, masif_coords, masif_neighbor)\n",
    "#         masif_feature_coord_nei_dict[pdbid] = (masif_feature, None, masif_neighbor)\n",
    "    except Exception as E:\n",
    "        print(E)\n",
    "        pass\n",
    "len(masif_feature_coord_nei_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outdir+'masif_feature_coord_nei_dict', 'wb') as f:\n",
    "    pickle.dump(masif_feature_coord_nei_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420 420 420  52 \n"
     ]
    }
   ],
   "source": [
    "am_dict = {}\n",
    "aa_dict = {}\n",
    "at_dict = {}\n",
    "for i in table.index:\n",
    "    print(len(am_dict), len(aa_dict), len(at_dict), \"\\r\", end=\"\")\n",
    "    pdbid = table.loc[i, 'pdbid']\n",
    "    anchor_coords = anchor_dict[pdbid]\n",
    "    _, atom_coords, _ = atom_dict[pdbid]\n",
    "    _, masif_coords, _ = masif_feature_coord_nei_dict[pdbid]\n",
    "#     try:  \n",
    "    # aa\n",
    "    aa_dist = pairwise_distances(anchor_coords, anchor_coords)\n",
    "    sele = np.where(aa_dist<=6)\n",
    "    i = torch.LongTensor(np.vstack(sele))\n",
    "    v = torch.FloatTensor(aa_dist[sele])\n",
    "    aa_sparse = torch.sparse.FloatTensor(i, v, torch.Size([aa_dist.shape[0], aa_dist.shape[1]]))\n",
    "    aa_dict[pdbid] = aa_sparse   \n",
    "\n",
    "    # am\n",
    "    am_dist = pairwise_distances(anchor_coords, masif_coords)\n",
    "    sele = np.where(am_dist<=6)\n",
    "    i = torch.LongTensor(np.vstack(sele))\n",
    "    v = torch.FloatTensor(am_dist[sele])\n",
    "    am_sparse = torch.sparse.FloatTensor(i, v, torch.Size([am_dist.shape[0], am_dist.shape[1]]))\n",
    "    am_dict[pdbid] = am_sparse\n",
    "\n",
    "    # at\n",
    "    at_dist = pairwise_distances(anchor_coords, atom_coords)\n",
    "    sele = np.where(at_dist<=6)\n",
    "    i = torch.LongTensor(np.vstack(sele))\n",
    "    v = torch.FloatTensor(at_dist[sele])\n",
    "    at_sparse = torch.sparse.FloatTensor(i, v, torch.Size([at_dist.shape[0], at_dist.shape[1]]))\n",
    "    at_dict[pdbid] = at_sparse\n",
    "\n",
    "#     except:\n",
    "#         pass\n",
    "    \n",
    "print(len(am_dict), len(aa_dict), len(at_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outdir+'am_dict', 'wb') as f:\n",
    "    pickle.dump(am_dict, f)\n",
    "with open(outdir+'aa_dict', 'wb') as f:\n",
    "    pickle.dump(aa_dict, f)\n",
    "with open(outdir+'at_dict', 'wb') as f:\n",
    "    pickle.dump(at_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymol\n",
    "import numpy as np\n",
    "\n",
    "def get_ligand_counts_coords(filename, chains, ligand_list, removeHs=True):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    filenameï¼špath+name of pdb file\n",
    "    ligand_list: list of ligand codes (3-letter IDs)\n",
    "    removeHs: whether to removeHs from the coordinates\n",
    "\n",
    "    Output:\n",
    "    count_dict: key: ligand id, value: number of occurrence\n",
    "    coord_dict: key: ligand id + chain + residue id, value: n*3 numpy array of compound coordinates\n",
    "    \"\"\"\n",
    "    pymol.cmd.reinitialize()\n",
    "    pymol.cmd.load(filename)\n",
    "    if removeHs:\n",
    "        pymol.cmd.remove('hydro')\n",
    "    \n",
    "    protein_coords = []\n",
    "    pymol.cmd.iterate_state(-1, \"chain \"+\"+\".join([x for x in chains])+\" and not het\", \"protein_coords.append((x,y,z))\", space=locals())\n",
    "\n",
    "    count_dict, coord_dict = {}, {}\n",
    "    list_tabu = [\"HOH\", \"DOD\", \"WAT\", \"NAG\", \"MAN\", \"UNK\", \"GLC\", \"ABA\", \"MPD\", \"GOL\", \"SO4\", \"PO4\", '', 'U', 'HEM', 'PI']\n",
    "    list_tabu += ['ASN', \"GLY\", \"ALA\", \"PRO\", \"VAL\", \"LEU\", \"ILE\", \"MET\", \"PHE\", \"TYR\", \"TRP\", \"SER\", \"THR\", \"CYS\", \\\n",
    "                 \"GLN\", \"LYS\", \"HIS\", \"ARG\", \"ASP\", \"GLU\"]\n",
    "    list_ligand_ok = set()\n",
    "    for ligand in ligand_list:\n",
    "        if ligand in list_tabu:\n",
    "            continue\n",
    "        resi_set = set()\n",
    "        ligand = ligand.upper()\n",
    "        pymol.cmd.iterate('resname {}'.format(ligand), \"resi_set.add(chain+'_'+resi)\", space=locals())\n",
    "        count_dict[ligand] = 0\n",
    "        for chain_resi in resi_set:\n",
    "            chain, resi = chain_resi.split('_')\n",
    "            pymol.cmd.select('{}_{}'.format(ligand, chain_resi), 'chain {} and resi {}'.format(chain, resi))\n",
    "            coords = []\n",
    "            pymol.cmd.iterate_state(-1, '{}_{}'.format(ligand, chain_resi), \"coords.append((x,y,z))\", space=locals())\n",
    "            if len(coords) < 5:\n",
    "                continue\n",
    "            coords = np.array(coords) \n",
    "            if pairwise_distances(protein_coords, coords).min() < 1.5:\n",
    "                continue\n",
    "            if pairwise_distances(protein_coords, coords).min() > 4:\n",
    "                continue\n",
    "            if pairwise_distances(protein_coords, np.mean(coords, 0, keepdims=True)).min() > 5.5:\n",
    "                continue\n",
    "            coord_dict['{}_{}'.format(ligand, chain_resi)] = coords\n",
    "            count_dict[ligand] += 1\n",
    "            list_ligand_ok.add(ligand)\n",
    "    return count_dict, coord_dict, list(list_ligand_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PyMOL not running, entering library mode (experimental)\n",
      "419  \r"
     ]
    }
   ],
   "source": [
    "# os.mkdir(\"ligand_coords\")\n",
    "dict_ligand_coords = {}\n",
    "dict_num_lig = {}\n",
    "for i in table.index:\n",
    "    pdbid = table.loc[i, 'pdbid']\n",
    "    if pdbid in dict_new_name:\n",
    "        newid = dict_new_name[pdbid]\n",
    "    else:\n",
    "        newid = pdbid\n",
    "    chains = table.loc[i, 'eval_chains']\n",
    "    if isinstance(table.loc[i, 'ligand'], str) and len(table.loc[i, 'ligand']) > 0:\n",
    "        list_ligand = table.loc[i, 'ligand'].split(',')\n",
    "    else:\n",
    "        list_ligand = []\n",
    "    if os.path.exists('MasifOutput/00-raw_pdbs/fixed_{}.pdb'.format(newid)):\n",
    "        filename = 'MasifOutput/00-raw_pdbs/fixed_{}.pdb'.format(newid)\n",
    "    elif os.path.exists('MasifOutput/00-raw_pdbs/{}.pdb'.format(newid)):\n",
    "        filename = 'MasifOutput/00-raw_pdbs/{}.pdb'.format(newid)\n",
    "    else:\n",
    "        print(\"NO pdb file\", pdbid)\n",
    "        continue\n",
    "    count_dict, coord_dict, list_ligand = get_ligand_counts_coords(filename, chains, list_ligand)\n",
    "    dict_ligand_coords[pdbid] = coord_dict\n",
    "    #     np.save(\"ligand_coords/{}.npy\".format(pdbid), coord_dict)\n",
    "\n",
    "    if len(list_ligand) == 0:\n",
    "        table.loc[i, 'ligand_used'] = \"\"\n",
    "    else:\n",
    "        table.loc[i, 'ligand_used'] = \",\".join(list_ligand)\n",
    "    dict_num_lig[pdbid] = sum(count_dict.values())\n",
    "    table.loc[i, 'num_ligands'] = int(sum(count_dict.values()))\n",
    "    print(i, '\\r', end=\"\")\n",
    "table['num_ligands'] = np.array(table['num_ligands'], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dict_ligand_coords, open(outdir+\"ligand_coords_dict\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchor_dict = pickle.load(open('anchor_dict_thre'+str(da), \"rb\"))\n",
    "# dict_ligand_coords = pickle.load(open(\"ligand_coords_dict\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for i in dict_ligand_coords:\n",
    "#     print(i)\n",
    "    total += len(dict_ligand_coords[i])\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419      \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "348"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### save label\n",
    "label_dict = {}\n",
    "processed = 0\n",
    "for i in table.index:\n",
    "    print(i, \" \\r\", end='')\n",
    "    pdbid = table.loc[i, 'pdbid']\n",
    "    try:\n",
    "        anchor_coords = anchor_dict[pdbid]\n",
    "        cpd_coords = np.concatenate(list(dict_ligand_coords[pdbid].values()))\n",
    "\n",
    "        ag = pairwise_distances(anchor_coords, cpd_coords).min(axis=1)\n",
    "        label = (ag <= 4).astype(int)\n",
    "        label_dict[pdbid] = label\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "len(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outdir+'anchor_label_n4_dict_'+str(da), 'wb') as f:\n",
    "    pickle.dump(label_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save final dataset table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success_list 420\n"
     ]
    }
   ],
   "source": [
    "success_list = []\n",
    "for i in table.index:\n",
    "    print(i, \" \\r\", end='')\n",
    "    pdbid = table.loc[i, 'pdbid']\n",
    "    if pdbid not in anchor_dict:\n",
    "        continue\n",
    "    if pdbid not in atom_dict:\n",
    "        continue  \n",
    "    if pdbid not in masif_feature_coord_nei_dict:\n",
    "        continue  \n",
    "    if pdbid not in am_dict:\n",
    "        continue  \n",
    "    if pdbid not in at_dict:\n",
    "        continue  \n",
    "    if pdbid not in aa_dict:\n",
    "        continue     \n",
    "#     if table.loc[i, 'num_ligands'] == 0:\n",
    "#         continue\n",
    "#     if pdbid not in label_dict:\n",
    "#         continue  \n",
    "    \n",
    "    success_list.append(i)\n",
    "print(\"success_list\", len(success_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420\n",
      "420\n"
     ]
    }
   ],
   "source": [
    "print(table.shape[0])\n",
    "table = table.loc[success_list]\n",
    "print(table.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.to_csv(outdir+\"coach420_table_pocket_full.tsv\", sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
